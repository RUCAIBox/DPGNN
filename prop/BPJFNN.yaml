# Data
min_word_cnt: 5
geek_longsent_len: 64
job_longsent_len: 192

# Model
embedding_size: 128
hidden_size: 128
dropout: 0.2
num_layers: 1

# Training
learning_rate: 0.0001

# General
train_batch_size: 2048
eval_batch_size: 4096
